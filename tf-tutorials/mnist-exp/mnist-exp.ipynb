{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________________________________________________________________\n",
    "This notebook follows the \"Deep MNIST for Experts\" tutorial on the TensorFlow website; at the time of \n",
    "writing this is located at https://www.tensorflow.org/versions/r0.8/tutorials/mnist/pros/index.html. \n",
    "This notebook contains a simple deep convolutional MNIST classifier.\n",
    "_______________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# load MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "\"\"\"start TensorFlow InteractiveSession - connects to C++ backend\n",
    "The common usage for TensorFlow programs is to first create a \n",
    "graph and then launch it in a session. Here we instead use the \n",
    "convenient 'InteractiveSession' class, which makes TensorFlow\n",
    "more flexible about how you structure the code. It allows you to\n",
    "interleave operations which build a computation graph with ones\n",
    "that run the graph.\"\"\"\n",
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Softmax Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9092\n"
     ]
    }
   ],
   "source": [
    "# create placeholders for input and output\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "# note that the 'shape' argument to placeholder is optional, but it allows for error-checking\n",
    "\n",
    "# create weights and biases for the model\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# initialize variables\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "# define model\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "# define cost function\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "\n",
    "# define one step of gradient descent; this adds new operations to the computation graph\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\"\"\"The returned operation train_step, when run, will apply the gradient descent updates to the\n",
    "parameters. Training the model can therefore be accomplished by repeatedly running train_step\"\"\"\n",
    "\n",
    "# train model\n",
    "for i in range(1000):\n",
    "    # load 50 training examples for each training iteration\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    # use feed_dict to replace the placeholder tensors x and y_ with training examples\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1]})\n",
    "    \n",
    "# evaluate the model\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Build a Multilayer Convolutional Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create functions to initialize weights and biases\n",
    "\n",
    "# no negative weights since we'll be using ReLUs\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# constant initial biases are fine\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# create functions for convolution and pooling\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create placeholders for input and output\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "# note that the 'shape' argument to placeholder is optional, but it allows for error-checking\n",
    "\n",
    "# compute 32 features for each 5x5 patch\n",
    "W_conv1 = weight_variable([5, 5, 1, 32]) # x_pix, y_pix, channels, features\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "# reshape x to [examples, width, height, channels]\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "# convolve x_image with weight tensor, add bias and apply ReLU\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "\n",
    "# max pool\n",
    "h_pool1 = max_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Densely Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# image size now 7x7; add a fully-connected layer with 1024 neurons\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "# reshape the tensor from the pooling layer into a batch of vectors\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "# create a placeholder to use for dropout in training\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readout Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.08\n",
      "step 100, training accuracy 0.88\n",
      "step 200, training accuracy 0.92\n",
      "step 300, training accuracy 0.92\n",
      "step 400, training accuracy 1\n",
      "step 500, training accuracy 0.96\n",
      "step 600, training accuracy 1\n",
      "step 700, training accuracy 0.96\n",
      "step 800, training accuracy 0.98\n",
      "step 900, training accuracy 0.92\n",
      "step 1000, training accuracy 0.98\n",
      "step 1100, training accuracy 1\n",
      "step 1200, training accuracy 1\n",
      "step 1300, training accuracy 0.94\n",
      "step 1400, training accuracy 0.94\n",
      "step 1500, training accuracy 0.98\n",
      "step 1600, training accuracy 0.96\n",
      "step 1700, training accuracy 0.96\n",
      "step 1800, training accuracy 0.96\n",
      "step 1900, training accuracy 0.98\n",
      "test accuracy 0.9733\n"
     ]
    }
   ],
   "source": [
    "# define loss\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv), reduction_indices=[1]))\n",
    "\n",
    "# define one step of the optimization routine\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "# define accuracy of model\n",
    "correct_prediction = tf.equal(tf.argmax(y_, 1), tf.argmax(y_conv, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# intialize all variables\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "# train\n",
    "for i in range(2000):\n",
    "    # get batch of data for this training step\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    \n",
    "    # print training updates\n",
    "    if i%100 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x: batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "        print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "        \n",
    "    # one step of optimization routine\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "    \n",
    "# print test accuracy\n",
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
